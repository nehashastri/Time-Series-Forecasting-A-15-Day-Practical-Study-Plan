# Use Cases by Industry

Forecasting is used in virtually every industry. The type of model that works best can vary by domain, data characteristics, and what is being forecast. Here’s a table of some industries, typical forecasting targets, and models often suited:

| Domain | Forecast Target | Common Effective Models |
|---|---|---|
| Retail | Daily or weekly sales for products/stores. Often multiple seasonal patterns (weekly, yearly), promotions, holiday effects. | Prophet (or seasonal ARIMA) to capture trend/seasonality/holidays, XGBoost/LightGBM with promo and weather features for store-item demand, LSTM if huge data (e.g. many stores) to capture complex patterns. <br>(Prophet, XGBoost, LSTM) are common. Prophet provides quick interpretable baseline[21]. Tree-based models handle price/promotion elasticities well. LSTM if looking for cross-store patterns or high-frequency intra-day data. |
| Finance | Stock prices, returns, volatility, risk metrics (often high-frequency or daily). Financial time series are noisy, often non-stationary, with possible regime changes. | ARIMA/GARCH for volatility (ARIMA for returns mean, GARCH for volatility clustering). LSTM and Transformer models are researched for stock price movement (trying to capture long-term dependencies or microstructure patterns)[71]. However, for pricing, simple models often do as well as complex due to noise. Also Prophet sometimes for things like market trends (if seasonal effects like day-of-week in crypto). <br>(ARIMA, LSTM, Transformer). E.g., ARIMA might be used for short-term interest rate forecasting, LSTM for detecting patterns in large volumes of historical data, Transformer (Informer, etc.) being state-of-art in financial forecasting research for long sequences. |
| Energy | Load (power demand) forecasting, generation (solar/wind output). Often hourly data with daily and weekly seasonality and weather dependence. | SARIMA and Exponential Smoothing (Holt-Winters) historically are very strong for load (since pattern is regular). TCN/Transformer models now achieve top performance for long-term load forecasting by capturing multiple seasonalities and weather effects. XGBoost with weather features also popular in Kaggle energy comps. For solar/wind, include weather forecasts and use ML/DL (maybe an ensemble of physical model + ML). <br>(SARIMA, TCN, Transformer). For example, a TCN could model short and long-term cycles in electricity usage better than a manual seasonal model, and a Transformer like Autoformer can handle a week’s worth of hourly forecasting with ease. |
| Healthcare | Patient arrivals, hospital admissions, disease incidence (often weekly or monthly; sometimes seasonal patterns like flu season). | Prophet is often used for its ability to include multiple seasonalities and holiday effects (e.g., fewer people schedule surgeries on holidays, etc.). GRU/LSTM for when there are multiple related time series (like admissions in multiple departments or multiple hospitals – a deep model can share info like flu trends among them). Also ARIMA for single hospital census forecasting. Healthcare data can have trend shifts (e.g., due to policy changes), so models that can adapt (like Prophet’s changepoints) are useful. <br>(Prophet, GRU). GRU (Gated Recurrent Unit) is a lighter RNN compared to LSTM, often sufficient if data is moderately sized, to capture some temporal patterns beyond linear. |
| Web Traffic | Website page visits, ad impressions, click-through rates – often high-frequency (hourly) and subject to day-of-week seasonality, trends, and sometimes viral spikes. | XGBoost with time features (hour, day, etc.) has been very successful (e.g., winning solutions in web traffic forecasting competitions often used boosted trees with lots of features including lagged values and special event indicators). Transformer-based models (like Temporal Fusion Transformer, or a specialized Time Series Transformer) are now achieving SOTA on long-range web traffic forecasting by capturing long-term dependencies and sudden changes. Also Seasonal naive is a strong baseline (yesterday same hour or last week same hour can be hard to beat for near term). But for longer term, FB Prophet can give a baseline capturing weekly cycles and trend. <br>(XGBoost, Transformer). Transformers (like PatchTST or Informer) have been reported to excel on traffic datasets by learning periodic patterns over long windows[71]. |
| Manufacturing | Equipment failure prediction (time-to-failure), production throughput or inventory levels. Often multiple sensors (multivariate series) and irregular patterns (machine degradation). | XGBoost and other tree ensembles are widely used for predictive maintenance (classification/regression using sensor readings). They need good features (vibration trending, temperature anomalies). For pure time series of metrics, TCN or LSTM can detect subtle patterns leading to failure (especially if plenty of run-to-failure examples). For throughput which might have trends and cycles, TCN or ARIMA depending on complexity. Many manufacturing processes are nonlinear, so tree or neural nets handle it better than linear models. <br>(XGBoost, TCN). Example: predict if a machine will fail in next hour – XGBoost might use last 10 min stats as features, while a TCN could directly ingest the high-frequency signal. |
| (More examples) | Marketing: Forecasting campaign response or demand uplift (often short-term, use ML with marketing spend features). Transportation: Forecast traffic or Uber demand – lots of data, deep models (LSTMs, Graph neural nets for spatial). Climate: Forecast weather or climate indices – heavy use of state-space models and increasingly deep learning (but often hybrid with physical models). | In general, models that incorporate domain structure perform best (e.g., specialized neural nets with exogenous factors for weather, or hierarchical models for grouped forecasts). Always consider ensembles of statistical + ML for robust performance[60]. |

The above are generalized recommendations. Often a combination/hybrid is used. For instance, in retail, a common approach is: use Prophet to forecast baseline for each product, then use XGBoost on residuals to capture promo lifts. In finance, use ARIMA for long-term trend and an LSTM for short-term micro-fluctuations and ensemble them.
Another perspective: sometimes operational constraints pick model – if interpretability is crucial (healthcare decisions, finance risk) you lean statistical or simpler ML. If data is extremely plentiful and accuracy is king (e.g. predicting clicks at Google scale), you lean deep learning.
Finally, industry use-cases also highlight that one size doesn’t fit all. Always evaluate a few approaches. And consider using domain knowledge: e.g., in energy, incorporate known holidays or daylight savings effects in any model; in healthcare, incorporate day-of-week surgery schedules, etc. The “best model” often is one that effectively leverages domain specifics either through features or model structure.

---

# Modern Tools and Libraries

The forecasting ecosystem has grown. Here are some notable tools and libraries that can make your life easier:

- **statsmodels:** A Python library that includes classical stats techniques. For time series, it has ARIMA/SARIMAX, ETS (ExponentialSmoothing), state space models, VAR for multivariate, etc. It’s great for statistical modeling and analysis. E.g., SARIMAX class for ARIMA, ExponentialSmoothing for Holt-Winters. Statsmodels also provides diagnostic plots and statistical tests (ADF, etc.)[73]. Use statsmodels when you need tried-and-true models or to inspect results (it gives summary output with confidence intervals for coefficients which is nice for interpretation).

- **prophet (Facebook Prophet):** As discussed, an easy-to-use framework for additive models with trend and seasonality[21]. It requires a dataframe with ‘ds’ (date) and ‘y’ (value) and with a few lines you get a forecast and components plots. Great for quick benchmarking and business-friendly analysis. Install as prophet. It handles missing dates and outliers reasonably. Prophet now (as of 2023) is somewhat maintained by community (original team moved on), but it’s still widely used.

- **pmdarima (auto-ARIMA):** This extends statsmodels by providing an auto_arima function that automatically performs differencing tests and tries various p, d, q (and seasonal P, D, Q) combinations to find the best model (by AIC/BIC)[17]. Very handy to not manually tune ARIMA. Example: pm.auto_arima(y, seasonal=True, m=12) will return an ARIMA model for monthly data with seasonality.

- **Darts:** A comprehensive Python library by Unit8 that offers a unified interface to a variety of forecasting models – from ARIMA, ExponentialSmoothing, Prophet to ML models (RandomForest, LightGBM) and deep learning (RNNs, TCN, Transformer). It’s great because you can try many models with similar code and it handles data preprocessing internally. It also has support for probabilistic forecasts and ensembling. If you want to experiment with multiple approaches quickly, Darts is a good choice. For example, from darts.models import TCNModel or ARIMA etc., and you call .fit() and .predict(). It also supports multivariate series and covariates in some models.  
  (One potential drawback is it might abstract some details, but for most use it’s fine, and you can always go to lower-level if needed.)

- **tsfresh:** A Python library to automatically compute a huge number of time series features (statistics, Fourier coefficients, autocorrelation measures, etc.) from raw data[28]. Useful if you want to use ML but don’t want to manually craft features – you give it the series (or multiple series) and it gives you a feature matrix. You still should be careful to avoid using future data in features, but tsfresh works on fixed windows labeled. People often use it for classification tasks (like classify if a time series segment indicates a fault), but for forecasting you could use it to create lag features etc. It can produce hundreds of features, so usually followed by feature selection. Good for when domain knowledge is lacking and you want machine to brute-force feature creation.

- **Kats (Kat’s Awesome Time Series), by Facebook:** This is an all-in-one library that includes implementations of several models (including Prophet, ARIMA, VAR, etc.), plus detection of changepoints, feature extraction, etc. It’s somewhat like a toolkit containing multiple forecasting and anomaly detection capabilities. It also has a module for ensemble forecasts. Kats isn’t as widely used as Prophet or Darts, but it’s useful if you want a single library to do many things – e.g., quickly apply a battery of models to see which performs best. It also can do hyperparameter tuning via its AutoML module for forecasting.

- **Nixtla’s libraries:** Nixtla is a group focused on open-source forecasting. Notably:  
  - **StatsForecast:** A high-performance library (in pure Python/Numba) implementing many statistical models (ARIMA, ETS, Theta, etc.) in a fast way for large-scale forecasting[74][75]. It’s meant for forecasting many series efficiently (with vectorized operations). Also includes some novel models (AutoTheta, etc.). It’s great if you have thousands of series and want to apply classic models quickly.  
  - **NeuralForecast:** Contains implementations of cutting-edge neural models (N-BEATS, N-HiTS, TFT, Informer, Autoformer, PatchTST, etc.) in an easy API[76]. If you want state-of-the-art deep learning models without implementing them, this is the go-to. It’s especially geared for researchers or advanced practitioners wanting to try new papers’ models. They often release trained models and tutorials.  
  - **MLForecast:** Nixtla also has MLForecast, which helps building features (lags, rolling stats) and training sklearn/XGBoost models for forecasting easily, again aimed at large scale.  

  Nixtla’s tools are quite new and actively developed, focusing on performance and ease for core tasks. They also integrate with Ray (for parallel computing) so you can distribute forecasting tasks easily[77].

- **GluonTS (Amazon):** A library in Python (and underlying MXNet for modeling, though now also supports PyTorch via a wrapper called PyTorchTS) that implements many advanced probabilistic forecasting models (DeepAR, Prophet, GP Forecaster, etc.). It’s especially strong on probabilistic forecasting (generating samples, quantiles). If you need full distribution forecasts (not just point), GluonTS is a solid choice. It also allows combining models easily and evaluating them on standard datasets.

- **sktime (Python):** An extension of scikit-learn for time series. It provides a unified interface to classical forecasters (ARIMA, ETS, etc.), and also time series classifiers and transformers. It’s academically well-structured. If you like scikit’s API design, sktime will feel familiar (fit/predict API for forecasters). They also have some automations and novel approaches like reduction from forecasting to regression.

- **PyCaret TS:** PyCaret (a low-code ML library) has a time series module that can do automated model training and ensembling for forecasting. It's relatively new but can train multiple models and output best one with few lines of code. Good for quick baselines if you prefer minimal coding.

- **Others:**  
  - TensorFlow Probability and TorchTS if you want to build custom probabilistic models.  
  - Orbit (Uber): for Bayesian time series models (like a Bayesian version of Prophet and other models).  
  - Azure AutoML, Google AutoML Tables can also do time series forecasting by treating it as tabular (they automatically generate lags etc.), which is useful if you are in those ecosystems and want an automated solution.  

Choosing a library: If you’re doing a one-off project and comfortable coding, statsmodels + a bit of manual feature work for ML covers a lot. If you want to rapidly prototype many models, Darts or sktime are great. If you have huge scale or want SOTA accuracy, Nixtla’s libraries are impressive. For enterprise integration, sometimes using cloud services (AWS Forecast, for instance, which is a managed service that tries multiple algorithms internally) could be considered, but those are black-box and not open-source.  

Finally, keep an eye on latest trends: In 2024-2025, there’s a rise of "foundation models" for time series (e.g., Salesforce’s TimeSeriesGPT mention, Nixtla’s TimeGPT[38]). These are large pre-trained models analogous to GPT for sequences. They promise zero-shot forecasting (where model can forecast new series without training on them, having learned generic time series patterns). It’s an emerging area – for now not widespread in industry, but might become influential.  
Also, libraries like H2O AutoML might incorporate time series modules, and things like Prophet 2 are in the works by community possibly addressing Prophet limitations (the 2023 update note hints at Prophet future plans[78]).

In summary, the toolbox is rich – leverage these libraries to avoid reinventing the wheel. They handle a lot of the heavy lifting (from data processing to model implementations), letting you focus on the problem-specific parts. Always validate results and understand what the library is doing (especially automated ones), but they can significantly speed up development and deliver strong results.

---

# Additional Trends (2025)

- **Forecasting Competitions & Benchmarks:** The M5 competition (Walmart data) in 2020 showed hybrid approaches (stat + ML) winning. The M4 competition earlier showed combination of statistical methods outperforming fancy ML in many cases[69]. However, newer competitions like M6 (financial forecasting) are exploring how humans and algorithms combine. Keep an eye on these, as they often steer the community on what works.

- **Intermittent demand forecasting:** New specialized models (e.g., Croston’s method improvements, or deep learning approaches) for lumpy demand are being developed – this is crucial in retail/warehouse (lots of zeros then a big order). Tools like ForecastingETS in StatsForecast have Croston and SBA methods.

- **Responsible AI in forecasting:** People are looking at how bias and fairness apply in forecasting decisions (less about the model bias, more about the impact – e.g., under-forecasting might lead to stockouts affecting certain regions). Interpretability and transparency are emphasized especially when forecasts drive automated decisions.

- **Big data and cloud computing:** It’s easier than ever to scale forecasting – you can use distributed computing to forecast thousands of series in parallel (Nixtla with Ray[75], or Spark). Cloud providers’ forecasting APIs (AWS Forecast, GCP Time Series Insights) allow upload data and get forecasts (they internally ensemble models, but at a cost and less control). If ease is needed and you trust a black box, those are options.

By staying updated (reading papers, community blogs, trying new libraries on a validation set), you can ensure you’re using the best tools for the job. But always verify on your data – sometimes an old simple method beats the latest fancy one for your particular scenario. Use the advanced libraries as assistants, but guide them with your knowledge of the data and business context.
